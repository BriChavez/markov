{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "# an nltk.FreqDist() is like a dictionary,\n",
    "\n",
    "# but it is ordered by frequency.\n",
    "\n",
    "# Also, nltk automatically fills the dictionary\n",
    "\n",
    "# with counts when given a list of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_brown = nltk.FreqDist(brown.words())\n",
    "\n",
    "list(freq_brown.keys())[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_brown.most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an nltk.ConditionalFreqDist() counts frequencies of pairs.\n",
    "\n",
    "# When given a list of bigrams, it maps each first word of a bigram\n",
    "\n",
    "# to a FreqDist over the second words of the bigram.\n",
    "\n",
    "\n",
    "cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(brown.words()))\n",
    "# conditions() in a ConditionalFreqDist are like keys()\n",
    "\n",
    "# in a dictionary\n",
    "\n",
    "cfreq_brown_2gram.conditions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# the cfreq_brown_2gram entry for \"my\" is a FreqDist.\n",
    "\n",
    "cfreq_brown_2gram[\"my\"]\n",
    "# here are the words that can follow after \"my\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first access the FreqDist associated with \"my\",\n",
    "\n",
    "# then the keys in that FreqDist\n",
    "\n",
    "cfreq_brown_2gram[\"my\"].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the 20 most frequent words to come after \"my\", with their frequencies\n",
    "\n",
    "cfreq_brown_2gram[\"my\"].most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an nltk.ConditionalProbDist() maps pairs to probabilities.\n",
    "\n",
    "# One way in which we can do this is by using Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This again has conditions() wihch are like dictionary keys\n",
    "\n",
    "cprob_brown_2gram.conditions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is what we find for \"my\": a Maximum Likelihood Estimation-based probability distribution,\n",
    "\n",
    "# as a MLEProbDist object.\n",
    "\n",
    "cprob_brown_2gram[\"my\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find the words that can come after \"my\" by using the function samples()\n",
    "\n",
    "cprob_brown_2gram[\"my\"].samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the probability of a particular pair:\n",
    "\n",
    "cprob_brown_2gram[\"my\"].prob(\"own\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "# We can also compute unigram probabilities (probabilities of individual words)\n",
    "\n",
    "freq_brown_1gram = nltk.FreqDist(brown.words())\n",
    "\n",
    "len_brown = len(brown.words())\n",
    "\n",
    "\n",
    "def unigram_prob(word):\n",
    "\n",
    "   return freq_brown_1gram[word] / len_brown\n",
    "\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The contents of cprob_brown_2gram, all these probabilities, now form a\n",
    "\n",
    "# trained bigram language model. The typical use for a language model is\n",
    "\n",
    "# to ask it for the probabillity of a word sequence\n",
    "\n",
    "# P(how do you do) = P(how) * P(do|how) * P(you|do) * P(do | you)\n",
    "\n",
    "prob_sentence = unigram_prob(\"how\") * cprob_brown_2gram[\"how\"].prob(\"do\") * cprob_brown_2gram[\"do\"].prob(\"you\") * \\\n",
    "    cprob_brown_2gram[\"you\"].prob(\"do\")\n",
    "\n",
    "# result: 1.5639033871961e-09\n",
    "\n",
    "###############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use a language model in another way:\n",
    "\n",
    "# We can let it generate text at random\n",
    "\n",
    "# This can provide insight into what it is that\n",
    "\n",
    "# the language model has been learning\n",
    "\n",
    "cprob_brown_2gram[\"my\"].generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can use this to generate text at random\n",
    "\n",
    "# based on a given text of bigrams.\n",
    "\n",
    "# Let's do this for the Sam \"corpus\"\n",
    "with open('./seuss_script.txt', 'r') as seuss_script:\n",
    "    # open txt file and read to string, string to lower\n",
    "    seussical = seuss_script.read()\n",
    "corpus = \"\"\"<s> I am Sam </s>\n",
    "\n",
    "<s> Sam I am </s>\n",
    "\n",
    "<s> I do not like green eggs and ham </s>\"\"\"\n",
    "\n",
    "words = corpus.split()\n",
    "\n",
    "cfreq_sam = nltk.ConditionalFreqDist(nltk.bigrams(words))\n",
    "\n",
    "cprob_sam = nltk.ConditionalProbDist(cfreq_sam, nltk.MLEProbDist)\n",
    "\n",
    "word = \"<s>\"\n",
    "\n",
    "for index in range(50):\n",
    "\n",
    "   word = cprob_sam[word].generate()\n",
    "\n",
    "   print(word, end=\" \")\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not a lot of variety. We need a bigger corpus.\n",
    "\n",
    "# What kind of genres do we have in the Brown corpus?\n",
    "\n",
    "brown.categories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try Science Fiction.\n",
    "\n",
    "cfreq_scifi = nltk.ConditionalFreqDist(\n",
    "    nltk.bigrams(brown.words(categories=\"science_fiction\")))\n",
    "\n",
    "cprob_scifi = nltk.ConditionalProbDist(cfreq_scifi, nltk.MLEProbDist)\n",
    "\n",
    "word = \"in\"\n",
    "\n",
    "for index in range(50):\n",
    "\n",
    "   word = cprob_scifi[word].generate()\n",
    "\n",
    "   print(word, end=\" \")\n",
    "\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# try this with other Brown corpus categories.\n",
    "\n",
    "# Here is how to do this with NLTK books:\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.book import *\n",
    "\n",
    "def generate_text(text, initialword, numwords):\n",
    "\n",
    "   bigrams = list(nltk.ngrams(text, 2))\n",
    "\n",
    "   cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(bigrams), nltk.MLEProbDist)\n",
    "\n",
    "   word = initialword\n",
    "\n",
    "   for i in range(numwords):\n",
    "\n",
    "     print(word, end = \" \")\n",
    "\n",
    "     word = cpd[ word].generate()\n",
    "\n",
    "   print(word)\n",
    "\n",
    "# Holy Grail\n",
    "\n",
    "generate_text(text6, \"I\", 100)\n",
    "\n",
    "# sense and sensibility\n",
    "\n",
    "generate_text(text2, \"I\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "\n",
    "# Word bigrams are just pairs of words.\n",
    "\n",
    "\n",
    "\n",
    "with open('./seuss_script.txt', 'r') as seuss_script:\n",
    "    # open txt file and read to string, string to lower\n",
    "    seussical = seuss_script.read()\n",
    "\n",
    "words_punct = seussical.split()\n",
    "\n",
    "\n",
    "# count bigrams\n",
    "\n",
    "bigrams = {}\n",
    "\n",
    "\n",
    "\n",
    "# strip all punctuation at the beginning and end of words, and\n",
    "\n",
    "# convert all words to lowercase.\n",
    "\n",
    "\n",
    "words = [w.strip(string.punctuation).lower() for w in words_punct]\n",
    "\n",
    "# add special START, END tokens\n",
    "\n",
    "words = [\"START\"] + words + [\"END\"]\n",
    "\n",
    "for index, word in enumerate(words):\n",
    "\n",
    "    if index < len(words) - 1:\n",
    "\n",
    "        # we only look at indices up to the\n",
    "\n",
    "        # next-to-last word, as this is\n",
    "\n",
    "        # the last one at which a bigram starts\n",
    "\n",
    "        w1 = words[index]\n",
    "\n",
    "        w2 = words[index + 1]\n",
    "\n",
    "        # bigram is a tuple,\n",
    "\n",
    "        # like a list, but fixed.\n",
    "\n",
    "        # Tuples can be keys in a dictionary\n",
    "\n",
    "        bigram = (w1, w2)\n",
    "\n",
    "        if bigram in bigrams:\n",
    "\n",
    "            bigrams[bigram] = bigrams[bigram] + 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            bigrams[bigram] = 1\n",
    "\n",
    "        # or, more simply, like this:\n",
    "\n",
    "        # bigrams[bigram] = bigrams.get(bigram, 0) + 1\n",
    "\n",
    "# sort bigrams by their counts\n",
    "\n",
    "sorted_bigrams = sorted(\n",
    "    bigrams.items(), key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "for bigram, count in sorted_bigrams:\n",
    "\n",
    "    print(bigram, \":\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "nltk.download('movie_reviews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def token_to_words(str):\n",
    "    f = StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        for i in str:\n",
    "            regex_of_word = re.findall('([\\w]{0,})', i)\n",
    "            regex_of_word = [x for x in regex_of_word if x is not '']\n",
    "            for word in regex_of_word:\n",
    "                print(regex_of_word)\n",
    "        words = (f.getvalue()).split('\\n')\n",
    "\n",
    "\n",
    "words = token_to_words(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/fossa/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/share/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28530/2393994719.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# seussical = seussical.lower()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# green_eggs = re.findall(r\"[\\w']+|[.,!?;]\", seussical)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseussical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/fossa/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/share/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk \n",
    "import nltk.data\n",
    "seuss_script = open('./seuss_script.txt', 'r')\n",
    "    # open txt file and read to string, string to lower\n",
    "seussical = seuss_script.read()\n",
    "# seussical = seussical.lower()\n",
    "# green_eggs = re.findall(r\"[\\w']+|[.,!?;]\", seussical)\n",
    "tokenlist = word_tokenize(seussical)\n",
    "\n",
    "\n",
    "# print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'green': True, 'eggs': False, 'and': True, 'ham': False, 'i': True, 'am': True, 'sam': False, '.': True, 'that': True, '!': True, 'do': True, 'not': True, 'like': True, 'would': True, 'you': True, '?': True, 'them': True, ',': True, 'here': True, 'or': True, 'there': True, 'anywhere': False, 'in': True, 'a': True, 'house': True, 'then': True, 'with': True, 'mouse': True, 'eat': False, 'box': True, 'fox': True, 'could': True, 'car': True, 'they': True, 'are': True, 'may': True, 'will': True, 'see': True, 'tree': True, 'let': True, 'me': True, 'be': True, 'train': True, 'on': True, 'say': True, 'the': True, 'dark': True, 'rain': True, 'goat': False, 'boat': False, 'so': True, 'try': False, 'if': True, 'good': True, 'thank': False}\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/fossa/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/share/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28530/193921965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# print(features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtokenlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseussical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mseuss_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseuss_books\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/dont_worry_about_it/dr_seuss_markov/venv/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/fossa/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/share/nltk_data'\n    - '/home/fossa/data/dont_worry_about_it/dr_seuss_markov/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "with open('./seuss_script.txt', 'r') as seuss_script:\n",
    "    # open txt file and read to string, string to lower\n",
    "    seussical = seuss_script.read()\n",
    "    seussical = seussical.lower()\n",
    "\n",
    "with open('extra_text.txt', 'r') as lots_o_books:\n",
    "    # open txt file and read to string, string to lower\n",
    "    read_books = lots_o_books.read()\n",
    "\n",
    "seuss_books = re.findall(r\"[\\w']+|[.,!?;]\", read_books)\n",
    "green_eggs = re.findall(r\"[\\w']+|[.,!?;]\", seussical)\n",
    "\n",
    "green_egg_words = []\n",
    "\n",
    "for word in green_eggs:\n",
    "    green_egg_words.append(word)\n",
    "\n",
    "green_egg_words = nltk.FreqDist(green_egg_words)\n",
    "\n",
    "common_words = list(green_egg_words.keys())\n",
    "# print(common_words)\n",
    "\n",
    "\n",
    "def find_features(seuss_books):\n",
    "    words = set(seuss_books)\n",
    "    features = {}\n",
    "    for w in common_words:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "print(find_features(seuss_books))\n",
    "\n",
    "# print(features)\n",
    "tokenlist = word_tokenize(seussical)\n",
    "\n",
    "seuss_training = seuss_books\n",
    "# classifier = nltk.NaiveBayesClassifier.train(seuss_training)\n",
    "# # random.shuffle(documents)\n",
    "# print(\"Classifier accuracy percent:\",\n",
    "#       (nltk.classify.accuracy(classifier, seuss_training))*100)\n",
    "# print(documents[1])\n",
    "\n",
    "# all_words = []\n",
    "# for w in movie_reviews.words():\n",
    "#     all_words.append(w.lower())\n",
    "\n",
    "# all_words = nltk.FreqDist(all_words)\n",
    "# print(all_words.most_common(15))\n",
    "# print(all_words[\"stupid\"])\n",
    "# # Mostly the same as before, only with now a new variable, word_features, which contains the top 3, 000 most common words\n",
    "# word_features = list(all_words.keys())[:3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we're going to build a quick function that will find these top 3,000 words in our positive and negative documents, marking their presence as either positive or negative:\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Next, we can print one feature set like:\n",
    "print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Then we can do this for all of our documents, saving the feature existence booleans and their respective positive or negative categories by doing:\"\"\"\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Stop Words removal:\n",
    "# When we use the features from a text to model, we will encounter a lot of noise. These are the stop words like the, he, her, etc… which don’t help us and , just be removed before processing for cleaner processing inside the model. With NLTK we can see all the stop words available in the English language.\n",
    "\n",
    "\n",
    "print(stopwords.words(\"english\"))\n",
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(“english”)]\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stemming:\n",
    "In our text we may find many words like playing, played, playfully, etc… \n",
    "which have a root word, play all of these convey the same meaning. So we \n",
    "can just extract the root word and remove the rest. Here the root word formed i\n",
    "s called ‘stem’ and it is not necessarily that stem needs to exist and have a meaning.\"\"\"\n",
    "#  Just by committing the suffix and prefix, we generate the stems.\n",
    "# NLTK provides us with PorterStemmer LancasterStemmer and SnowballStemmer packages\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Lemmatization:\n",
    "We want to extract the base form of the word here. The word extracted \n",
    "here is called Lemma and it is available in the dictionary. We have the \n",
    "WordNet corpus and the lemma generated will be available in this corpus. \n",
    "NLTK provides us with the WordNet Lemmatizer that makes use of the WordNet \n",
    "Database to lookup lemmas of words.\"\"\"\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parse tree or Syntax Tree generation :\n",
    "We can define grammar and then use NLTK RegexpParser to extract all parts of \n",
    "speech from the sentence and draw functions to visualize it.\"\"\"\n",
    "\n",
    "# Import required libraries\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Example text\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "# Find all parts of speech in above sentence\n",
    "tagged = pos_tag(word_tokenize(sample_text))\n",
    "#Extract all parts of speech from any text\n",
    "chunker = RegexpParser(\"\"\"NP: {?*} #To extract Noun Phrases\n",
    "\t\t\t\t\t    P: {}\t\t\t #To extract Prepositions\n",
    "\t\t\t\t\t    V: {}\t\t\t #To extract Verbs\n",
    "\t\t\t\t\t    PP: {} #To extract Prepositional Phrases\n",
    "                        VP: { *} #To extract Verb Phrases\"\"\")\n",
    "# Print all parts of speech in above sentence\n",
    "output = chunker.parse(tagged)\n",
    "print(“After Extractingn”, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"POS Tagging:\n",
    "Part of Speech tagging is used in text processing to avoid confusion between \n",
    "two same words that have different meanings. With respect to the definition \n",
    "and context, we give each word a particular tag and process them. Two Steps \n",
    "are used here:\"\"\"\n",
    "\n",
    "# Tokenize text(word_tokenize).\n",
    "# Apply the pos_tag from NLTK to the above step.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "txt = \"Natural language processing is an exciting area.\"\n",
    "\" Huge budget have been allocated for this.\"\n",
    "# sent_tokenize is one of instances of\n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
    "tokenized = sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "  # Word tokenizers is used to find the words\n",
    "  # and punctuation in a string\n",
    "  wordsList = nltk.word_tokenize(i)\n",
    "  # removing stop words from wordList\n",
    "  wordsList = [w for w in wordsList if not w in stop_words]\n",
    "  # Using a Tagger. Which is part-of-speech\n",
    "  # tagger or POS-tagger.\n",
    "  tagged = nltk.pos_tag(wordsList)\n",
    "  print(tagged)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d415629578ab31ae194d9b03ba0f4bbf888c1f5b85b5b09dc8ec40167408b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
